<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Licheng Wen</title> <meta name="author" content="Licheng Wen"> <meta name="description" content="Personal website of Licheng Wen, a researcher in the field of autonomous driving and multi-agent systems. "> <meta name="keywords" content="wen licheng, licheng, jekyll, academic-website, personal website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A8%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://wenlc.cn/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" target="_blank" href="/assets/pdf/LichengWen_resume.pdf">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <style>.clustrmaps-container{display:flex;justify-content:center;align-items:center}.clustrmaps-globe{width:50%;height:auto;overflow:hidden}</style> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Licheng</span> Wen </h1> <p class="desc">Multimodal LLM Researcher</p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/my_photo.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="my_photo.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am currently a researcher at the <a href="https://www.shlab.org.cn/" rel="external nofollow noopener" target="_blank">Shanghai AI Laboratory</a>, where I also collaborate closely with the <a href="https://www.sii.edu.cn/" rel="external nofollow noopener" target="_blank">Shanghai Institute of Innovation</a>. <br>I received my M.Sc. degree from <a href="https://www.zju.edu.cn/english/" rel="external nofollow noopener" target="_blank">Zhejiang University</a> in 2022, where I was a member of the <a href="https://april.zju.edu.cn/" rel="external nofollow noopener" target="_blank">APRIL Lab</a>, advised by <a href="https://person.zju.edu.cn/yongliu" rel="external nofollow noopener" target="_blank">Dr. Yong Liu</a>. Prior to that, I obtained my bachelor‚Äôs degree from Zhejiang University in 2019.</p> <p>My research focuses on enabling AI agents to operate efficiently in practical and valuable scenarios, with an emphasis on allowing foundation models to learn and improve during <em>run-time</em> rather than solely at <em>design-time</em>. My prior work centered on addressing complex interaction challenges in autonomous driving. I am passionate about the future of artificial general intelligence (AGI) and excited to contribute to these rapidly advancing fields.</p> <h3 id="research-interests">Research Interests</h3> <ul> <li>AI Agents</li> <li>Multimodal Foundation Models</li> <li>Multi-Agent System</li> <li>Autonomous Driving</li> </ul> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 1, 2025</th> <td> üéâ Paper <a href="http://arxiv.org/abs/2405.15324" rel="external nofollow noopener" target="_blank">DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving</a> is accepted by <i>ICCV 2025</i> ! <br> </td> </tr> <tr> <th scope="row">Oct 2, 2024</th> <td> üéâ Paper <a href="http://arxiv.org/abs/2405.15324" rel="external nofollow noopener" target="_blank">Continuously Learning, Adapting, and Improving: A Dual-Process Approach to Autonomous Driving</a> is accepted by <i>NeurIPS 2024</i> ! <br> </td> </tr> <tr> <th scope="row">Apr 1, 2024</th> <td> üéâ Paper <a href="https://pjlab-adg.github.io/limsim_plus/" rel="external nofollow noopener" target="_blank">LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving</a> is aceepted by <i>IV 2024</i> ! </td> </tr> <tr> <th scope="row">Jan 17, 2024</th> <td> üéâ Paper <a href="https://pjlab-adg.github.io/DiLu/" rel="external nofollow noopener" target="_blank">DiLuüê¥: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models</a> is aceepted by <i>ICLR 2024</i> ! </td> </tr> <tr> <th scope="row">Jan 2, 2024</th> <td> üéâ Paper <a href="https://doi.org/10.1016/j.aap.2023.107445" rel="external nofollow noopener" target="_blank">How drivers perform under different scenarios: Ability-related driving style extraction for large-scale dataset</a> is published on <i>Journal of Accident Analysis &amp; Prevention</i>. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div> <div id="mei2025o2searchersearchingbasedagentmodel" class="col-sm-8"> <div class="title">O^2-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering</div> <div class="author"> Jianbiao Mei,¬†Tao Hu,¬†Daocheng Fu, and <span class="more-authors" title="click to view 11 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '11 more authors' ? 'Licheng Wen, Xuemeng Yang, Rong Wu, Pinlong Cai, Xinyu Cai, Xing Gao, Yu Yang, Chengjun Xie, Botian Shi, Yong Liu, Yu Qiao' : '11 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">11 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2505.16582</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.16582" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/KnowledgeXLab/O2-Searcher" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O2-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O2-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from model‚Äôs sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O2-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mei2025o2searchersearchingbasedagentmodel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mei, Jianbiao and Hu, Tao and Fu, Daocheng and Wen, Licheng and Yang, Xuemeng and Wu, Rong and Cai, Pinlong and Cai, Xinyu and Gao, Xing and Yang, Yu and Xie, Chengjun and Shi, Botian and Liu, Yong and Qiao, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2505.16582}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/drivearena.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="drivearena.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2024drivearena" class="col-sm-8"> <div class="title">DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving</div> <div class="author"> Xuemeng Yang*,¬†<em>Licheng Wen*</em>,¬†Yukai Ma*, and <span class="more-authors" title="click to view 11 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '11 more authors' ? 'Jianbiao Mei*, Xin Li*, Tiantian Wei*, Wenjie Lei, Daocheng Fu, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yong Liu, Yu Qiao' : '11 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">11 more authors</span> </div> <div class="periodical"> <em>2025 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2408.00415" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pjlab-adg.github.io/DriveArena/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Demo</a> <a href="https://github.com/pjlab-adg/DriveArena" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper presetns DriveArena, the first high-fidelity closed-loop simulation system designed for driving agents navigating in real scenarios. DriveArena features a flexible, modular architecture, allowing for the seamless interchange of its core components: Traffic Manager, a traffic simulator capable of generating realistic traf- fic flow on any worldwide street map, and World Dreamer, a high-fidelity conditional generative model with infinite autoregression. This powerful synergy empowers any driving agent capable of processing real-world images to navigate in DriveArena simulated environment. The agent perceives its surroundings through images generated by World Dreamer and output trajectories; then these trajectories are fed into Traffic Manager, achieving realistic interactions with other vehicles and producing a new scene lay- out. Finally, the latest scene layout is relayed back into World Dreamer, perpetuating the simulation cycle. This iterative process fosters closed-loop exploration within a highly realistic environment, providing a valuable platform for developing and evaluating driving agents across diverse and challenging scenarios. DriveArena signifies a substantial leap forward in leveraging generative image data for the driving simulatior, opening insights for closed-loop autonomous driving.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2024drivearena</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang*, Xuemeng and Wen*, Licheng and Ma*, Yukai and Mei*, Jianbiao and Li*, Xin and Wei*, Tiantian and Lei, Wenjie and Fu, Daocheng and Cai, Pinlong and Dou, Min and Shi, Botian and He, Liang and Liu, Yong and Qiao, Yu}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{2025 IEEE/CVF International Conference on Computer Vision (ICCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://pjlab-adg.github.io/DriveArena/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="wen2023dilu" class="col-sm-8"> <div class="title">DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models</div> <div class="author"> <em>Licheng Wen*</em>,¬†Daocheng Fu*,¬†Xin Li*, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>In The Eleventh International Conference on Learning Representations (ICLR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.16292" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://pjlab-adg.github.io/DiLu/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Demo</a> <a href="https://github.com/PJLab-ADG/DiLu" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu‚Äôs capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to instill knowledge-driven capability into autonomous driving systems from the perspective of how humans drive.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wen2023dilu</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wen*, Licheng and Fu*, Daocheng and Li*, Xin and Cai, Xinyu and Ma, Tao and Cai, Pinlong and Dou, Min and Shi, Botian and He, Liang and Qiao, Yu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Eleventh International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">demo</span> <span class="p">=</span> <span class="s">{https://pjlab-adg.github.io/DiLu/}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/on_the_road.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="on_the_road.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wen2024road" class="col-sm-8"> <div class="title">On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving</div> <div class="author"> <em>Licheng Wen*</em>,¬†Xuemeng Yang*,¬†Daocheng Fu*, and <span class="more-authors" title="click to view 14 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '14 more authors' ? 'Xiaofeng Wang*, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, Zheng Zhu, Shaoyan Sun, Yeqi Bai, Xinyu Cai, Min Dou, Shuanglu Hu, Botian Shi' : '14 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">14 more authors</span> </div> <div class="periodical"> <em>In ICLR 2024 Workshop on Large Language Model (LLM) Agents</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.05332" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/PJLab-ADG/GPT4V-AD-Exploration" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The development of autonomous driving technology depends on merging perception, decision, and control systems. Traditional strategies have struggled to understand complex driving environments and other road users‚Äô intent. This bottleneck, especially in constructing common sense reasoning and nuanced scene understanding, affects the safe and reliable operations of autonomous vehicles. The introduction of Visual Language Models (VLM) opens up possibilities for fully autonomous driving. This report evaluates the potential of GPT-4V(ision), the latest state-of-the-art VLM, as an autonomous driving agent. The evaluation primarily assesses the model‚Äôs ultimate ability to act as a driving agent under varying conditions, while also considering its capacity to understand driving scenes and make decisions. Findings show that GPT-4V outperforms existing systems in scene understanding and causal reasoning. It has the potential in handling unexpected scenarios, understanding intentions, and making informed decisions. However, limitations remain in direction determination, traffic light recognition, vision grounding, and spatial reasoning tasks, highlighting the need for further research. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wen2024road</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wen*, Licheng and Yang*, Xuemeng and Fu*, Daocheng and Wang*, Xiaofeng and Cai, Pinlong and Li, Xin and Ma, Tao and Li, Yingxuan and Xu, Linran and Shang, Dengke and Zhu, Zheng and Sun, Shaoyan and Bai, Yeqi and Cai, Xinyu and Dou, Min and Hu, Shuanglu and Shi, Botian}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ICLR 2024 Workshop on Large Language Model (LLM) Agents}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ITSC</abbr></div> <div id="wen2023limsim" class="col-sm-8"> <div class="title">LimSim: A Long-term Interactive Multi-scenario Traffic Simulator</div> <div class="author"> <em>Licheng Wen*</em>,¬†Daocheng Fu*,¬†Song Mao, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Pinlong Cai, Min Dou, Yikang Li' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2307.06648" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/playlist?list=PLNeNtm096CAyYD1JJnkQ4gMaoFSdFLn2y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/PJLab-ADG/LimSim" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>With the growing popularity of digital twin and autonomous driving in transportation, the demand for simulation systems capable of generating high-fidelity and reliable scenarios is increasing. Existing simulation systems suffer from a lack of support for different types of scenarios, and the vehicle models used in these systems are too simplistic. Thus, such systems fail to represent driving styles and multi-vehicle interactions, and struggle to handle corner cases in the dataset. In this paper, we propose LimSim, the Long-term Interactive Multi-scenario traffic Simulator, which aims to provide a long-term continuous simulation capability under the urban road network. LimSim can simulate fine-grained dynamic scenarios and focus on the diverse interactions between multiple vehicles in the traffic flow. This paper provides a detailed introduction to the framework and features of the LimSim, and demonstrates its performance through case studies and experiments. LimSim is now open source on GitHub.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wen2023limsim</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LimSim: A Long-term Interactive Multi-scenario Traffic Simulator}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wen*, Licheng and Fu*, Daocheng and Mao, Song and Cai, Pinlong and Dou, Min and Li, Yikang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/playlist?list=PLNeNtm096CAyYD1JJnkQ4gMaoFSdFLn2y}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/clcbs.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="clcbs.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1016/j.robot.2021.103997" class="col-sm-8"> <div class="title">CL-MAPF: Multi-Agent Path Finding for Car-Like robots with kinematic and spatiotemporal constraints</div> <div class="author"> <em>Licheng Wen</em>,¬†Yong Liu,¬†and¬†Hongliang Li</div> <div class="periodical"> <em>Robotics and Autonomous Systems</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2011.00441" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.youtube.com/watch?v=KThsX04ABvc" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/APRIL-ZJU/CL-CBS" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Multi-Agent Path Finding has been widely studied in the past few years due to its broad application in the field of robotics and AI. However, previous solvers rely on several simplifying assumptions. This limits their applicability in numerous real-world domains that adopt nonholonomic car-like agents rather than holonomic ones. In this paper, we give a mathematical formalization of the Multi-Agent Path Finding for Car-Like robots (CL-MAPF) problem. We propose a novel hierarchical search-based solver called Car-Like Conflict-Based Search to address this problem. It applies a body conflict tree to address collisions considering the shapes of the agents. We introduce a new algorithm called Spatiotemporal Hybrid-State A* as the single-agent planner to generate agents‚Äô paths satisfying both kinematic and spatiotemporal constraints. We also present a sequential planning version of our method, sacrificing a small amount of solution quality to achieve a significant reduction in runtime. We compare our method with two baseline algorithms on a dedicated benchmark and validate it in real-world scenarios. The experiment results show that the planning success rate of both baseline algorithms is below 50% for all six scenarios, while our algorithm maintains that of over 98%. It also gives clear evidence that our algorithm scales well to 100 agents in 300¬†m¬†√ó¬†300¬†m scenario and is able to produce solutions that can be directly applied to Ackermann-steering robots in the real world. The benchmark and source code are released in https://github.com/APRIL-ZJU/CL-CBS. The video of the experiments can be found on YouTube.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">10.1016/j.robot.2021.103997</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{CL-MAPF: Multi-Agent Path Finding for Car-Like robots with kinematic and spatiotemporal constraints}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Robotics and Autonomous Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{150}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{103997}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0921-8890}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.robot.2021.103997}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0921889021002530}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wen, Licheng and Liu, Yong and Li, Hongliang}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Multi-agent systems, Path planning, Mobile robots}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=KThsX04ABvc}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <h2 id="service" style="color: inherit;">Talks</h2> <p> On October 31st, 2024, I had the honor of presenting a talk titled <strong>"Empowering Automated Driving with LLMs: A Knowledge-driven Paradigm"</strong> to <a href="https://www.sae.org/" target="_blank" rel="noopener noreferrer">SAE International</a> as part of their AI Webinar series. </p> <div class="video-container"> <iframe width="100%" height="400" src="https://www.youtube.com/embed/SfW0quOEzLM?si=CU9a07laEphTYHKN" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> </div> <p></p> <p></p> <hr style="width: 100%; margin: 40px 0;"> <div class="social"> <div class="contact-icons"> <a href="https://scholar.google.com/citations?user=RNnjXTkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/zijinoier" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/licheng-wen" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/zijinoier" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a> </div> <div class="contact-note"> Email is the best way to reach me: wenlicheng [at] pjlab dot org dot cn </div> </div> </article> </div> <div class="clustrmaps-container"> <div class="clustrmaps-globe"> <p></p> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&amp;w=a&amp;t=tt&amp;d=DdK4bEleGev5v27ZMB_jXRGpdDwAE3KnulBHdoyl8Ps&amp;co=ffffff&amp;cmo=3acc3a&amp;cmn=ff5353&amp;ct=808080"></script> </div> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> ¬© Copyright 2025 Licheng Wen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.Last updated: September 09, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SE6TFLVDZE"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SE6TFLVDZE");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>